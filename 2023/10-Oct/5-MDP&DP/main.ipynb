{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inventory Optimization with Dynamic Programming in Less than 100 Python code\n",
    "\n",
    "### Part 3: Implementing the Dynamic Programming for Inventory Optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import pretty\n",
    "pretty.install()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In first part of series on Inventory Optimization, a Markov Process was covered. Essentilally Markov Process was introduced to provide model of the states, how to model the transition of states in Inventory Optimization problem.\n",
    "\n",
    "In the second part it was dicussed how fixes policy, can make Markoe reward Processs. The main goal of the second part was to follow Bellman Equation on how to find the Immediate reward and Value function for each state.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this article, we are going to summarize everything and couple Markov process and MArkov Reward Process with **Markov Decision Process**.\n",
    "\n",
    "Markov Decision Process is a  process, where all decsions are condidered for each state. In other words, we are going to consider all possible actions for each state.\n",
    "\n",
    "In Markov Reward Process , the decsion were fixed, and we were not considering all possible actions for each state. In MDP, we need to build a data structure that for each state, we need to consider all possible actions, and following the $(S_t,A_t)$, we need to know what will be $(S_{t+1}, R_{t+1})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of Markov Decision Process Data Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am giving you an example of MDP dictionary, where as you can see for each state, all possible actions need to be considred, and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MarkovDecProcessDict = {\"Current State A\":{\"Action 1\":{(\"NextS1fromAact1\", \"Reward1\"): \"PNextS1fromAact1\"\n",
    "                                           ,(\"NextS2fromAact1\", \"Reward2\"): \"PNextS2fromAact1\"},\n",
    "                                           \"Action 2\":{(\"NextS1fromAact2\", \"Reward2\"): \"PNextS1fromAact2\"\n",
    "                                           ,(\"NextS2fromAact2\", \"Reward2\"): \"PNextS2fromAact2\"}},\n",
    "                    \n",
    "                     \"Current State B\":{\"Action 1\":{(\"NextS1fromBact1\", \"Reward1\"): \"PNextS1fromBact1\"\n",
    "                                           ,(\"NextS2fromBact1\", \"Reward2\"): \"PNextS2fromBact1\"},\n",
    "                                           \"Action 2\":{(\"NextS1fromBact2\", \"Reward2\"): \"PNextS1fromBact2\"\n",
    "                                           ,(\"NextS2fromBact2\", \"Reward2\"): \"PNextS2fromBact2\"}}\n",
    "}\n",
    "\n",
    "for current_state, actions in MarkovDecProcessDict.items():\n",
    "    print(f\"Current State: {current_state}\")\n",
    "    \n",
    "    for action, transitions in actions.items():\n",
    "        print(f\"  Action: {action}\")\n",
    "        \n",
    "        for (next_state, reward), probability in transitions.items():\n",
    "            print(f\"  ({next_state},{reward}): {probability}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDP for Inventory Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first sarticle, we dicussed Markov Process for Inventory Optimization.In the second artcile we dicussed Markov reward Polciy for one **fixed policy** and found what are the state value function of state, if we follow that policy. But, then the question we asked is \"How to find the best policy\"?. To answer that question we are coing to markov Process and here we are building the dictionary, where for each state, we are considering all possible actions, and for each action, we are considering all possible states and rewards.\n",
    "\n",
    "This ditionary name is **MDP_dict** and I wrote hands-one code of it below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "\n",
    "# poisson is used to find pdf of Poisson distribution \n",
    "from scipy.stats import poisson\n",
    "\n",
    "from rich import pretty\n",
    "pretty.install()\n",
    "\n",
    "MDP_dict: Dict[tuple, Dict[tuple, tuple]] = {}\n",
    "\n",
    "user_capacity = 2\n",
    "user_poisson_lambda = 1\n",
    "holding_cost = 1\n",
    "missedcostumer_cost = 10\n",
    "\n",
    "\n",
    "\n",
    "for alpha in range(user_capacity+1):                            \n",
    "                                                               \n",
    "    for beta in range(user_capacity + 1 - alpha):\n",
    "        \n",
    "        # This is St, the current state\n",
    "        state = (alpha, beta)                                   \n",
    "\n",
    "        # This is initial inventory, total bike you have at 8AM \n",
    "        init_inv = alpha + beta                         \n",
    "        \n",
    "        # The beta1 is the beta in next state, irrespctive of current state (as the decsion policy is constant)\n",
    "        beta1 = user_capacity - init_inv\n",
    "        \n",
    "        base_reward = -alpha* holding_cost\n",
    "        # List of all possible demand you can get\n",
    "        \n",
    "        #dict1 = {}\n",
    "        action = {}\n",
    "        # Consider all possible actions\n",
    "        for order in range(user_capacity-init_inv +1):\n",
    "            \n",
    "            #action = {}\n",
    "            dict1 = {}\n",
    "            for i in range(init_inv +1):\n",
    "\n",
    "            # if initial demand can meet the deman\n",
    "                if i <= (init_inv-1):\n",
    "                \n",
    "                # probality of specifc demand can happen\n",
    "                    transition_prob = poisson.pmf(i,user_poisson_lambda)\n",
    "\n",
    "                    dict1[((init_inv - i, order), base_reward)] = transition_prob\n",
    "\n",
    "                         \n",
    "            # if initial demand can not meet the demand\n",
    "                else:\n",
    "                \n",
    "                    transition_prob = 1- poisson.cdf(init_inv -1, user_poisson_lambda)\n",
    "                \n",
    "                # probability of not meeting the demands\n",
    "                    transition_prob2 = 1- poisson.cdf(init_inv, user_poisson_lambda)\n",
    "                \n",
    "                # total reward\n",
    "                \n",
    "                    reward = base_reward - missedcostumer_cost*((user_poisson_lambda*transition_prob) - \\\n",
    "                                                  init_inv*transition_prob2)                \n",
    "\n",
    "                    dict1[((init_inv - i, order),reward)] = transition_prob\n",
    "\n",
    "                    #if state in MDP_dict:\n",
    "\n",
    "            action[order] = dict1\n",
    "\n",
    "        MDP_dict[state]= action\n",
    "\n",
    "MDP_dict\n",
    "# Constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(filename=\"Img/explainmDPdic.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Programming\n",
    "\n",
    "Richard Bellman (1950s) first coined the term called **Dynamic Programming**. Dynamic Programming is a method for solving complex problems by breaking them down into simpler subproblems. \n",
    "\n",
    "DP generally refers to genral theories of MArkov decion Process , and algorithm to find optimal policy in MDB , relying heaviliy on Bellman Equation.\n",
    "\n",
    "In the context of this artcile, we use term Dynamic Programming with the goal of finding the optimal policy for Inventory Optimization problem. There are genarlly two import DP alghrotithm namesl:\n",
    "\n",
    "-  Value Function Iteration Alghorithm (Bellman 1957)\n",
    "-  Policy Iteration Alghorithm (Howard 1960)\n",
    "\n",
    "In this article, we are going to focus on Policy Iteration Algorithm and we are going to implement it in Python.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration Algorithm for Inventory Optimization: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy iteration alghorithm is a method for finding the optimal policy for a given MDP. The algorithm is based on the following idea:\n",
    "\n",
    "- 1) Start with an initial policy $\\pi_0$.\n",
    "- 2) Evaluate the policy $\\pi_0$ by computing the state-value function $V^{\\pi_0}$.\n",
    "- 3) Improve the policy by acting greedily with respect to $V^{\\pi_0}$ to get a new policy $\\pi_1$.\n",
    "\n",
    "The algorithm iterates on the above steps until the policy converges (stopes to change). We are going to g through all three stages in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Start with Initial Policy\n",
    "\n",
    "Policy iteraton algorith needs a initial policy , which can be any arbitarary policy. In this article, we are going to use the policy that we found in the second article, which is the following:\n",
    "Initial policy:\n",
    "\n",
    "$$\\pi_0=C-(\\alpha + \\beta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the initial policy is that at each state of the inventory, we order the amount of $C-(\\alpha + \\beta)$, which is the difference between the capacity of the inventory and the sum of initial items in the inventory ($\\alpha$) and the ones will come tomorrow moorning ($\\beta$) .\n",
    "\n",
    "Here is the code for the initial policy:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_capacity_val = 2\n",
    "\n",
    "def policy_0_gen(user_capacity: int):\n",
    "        \n",
    "        # Generate an initial policy\n",
    "\n",
    "        return {(alpha, beta): user_capacity - (alpha + beta) \n",
    "                for alpha in range(user_capacity + 1) \n",
    "                for beta in range(user_capacity + 1 - alpha)}\n",
    "\n",
    "policy_0 = policy_0_gen(user_capacity_val)\n",
    "policy_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Evaluate the policy $\\pi_0$ by computing the state-value function $V^{\\pi_0}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would like to say that any MarkoW Decsion Process with fixed policy lead to a **implied** MArkov Rewad Process. So, like previous articel, if we have mArkov Reward Process, we can find the state value function for each state.\n",
    "In another word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(filename=\"Img/mdp_to_mrp.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function will get a **fixed policy**, then it will return a implied Markov Reward Process:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MRP_using_fixedPolicy(full_MDP, policy):\n",
    "        # Calculate the Markov Reward Process using a fixed policy\n",
    "        MRP_policy = {}\n",
    "        for state in full_MDP.keys():\n",
    "            action = policy[state]\n",
    "            MRP_policy[state] = full_MDP[state][action]\n",
    "        return MRP_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we can give the initial policy to the below function, and it will return the implied Markov Reward Process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MRP_p0=MRP_using_fixedPolicy(MDP_dict, policy_0)\n",
    "MRP_p0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the Markov Reward Process, it is quite easy to find Immediate Rewards for each state, and also the state value function for each state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_expected_immediate_rewards(MRP_policy):\n",
    "        # Calculate the expected immediate rewards from the MRP policy\n",
    "        E_immediate_R = {}\n",
    "        for from_state, value in MRP_policy.items():\n",
    "            expected_reward = sum(reward[1] * prob for (reward, prob) in value.items())\n",
    "            E_immediate_R[from_state] = expected_reward\n",
    "        return E_immediate_R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_ime_p0 = calculate_expected_immediate_rewards(MRP_p0)\n",
    "R_ime_p0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "def create_transition_probability_matrix(MRP_policy):\n",
    "        # Create the transition probability matrix\n",
    "        states = list(MRP_policy.keys())\n",
    "        num_states = len(states)\n",
    "        trans_prob = np.zeros((num_states, num_states))\n",
    "        df_trans_prob = pd.DataFrame(trans_prob, columns=states, index=states)\n",
    "        for i, from_state in enumerate(states):\n",
    "            for j, to_state in enumerate(states):\n",
    "                for (new_state, reward) in MRP_policy.get(from_state, {}):\n",
    "                    if new_state == to_state:\n",
    "                        probability = MRP_policy[from_state].get((new_state, reward), 0.0)\n",
    "                        df_trans_prob.iloc[i, j] = probability\n",
    "                        \n",
    "        return df_trans_prob\n",
    "\n",
    "def calculate_state_value_function(trans_prob_mat, expected_immediate_rew, gamma=0.9):\n",
    "        # Calculate the state value function\n",
    "        states = list(expected_immediate_rew.keys())\n",
    "        R_exp = np.array(list(expected_immediate_rew.values()))\n",
    "        val_func_vec = np.linalg.solve(np.eye(len(R_exp)) - gamma * trans_prob_mat, R_exp)\n",
    "        MarkRevData = pd.DataFrame({'Expected Immediate Reward': R_exp, 'Value Function': val_func_vec}, \n",
    "                                   index=states)\n",
    "        return MarkRevData\n",
    "\n",
    "trans_prob_p0 = create_transition_probability_matrix(MRP_p0)\n",
    "\n",
    "state_val_p0 = calculate_state_value_function(trans_prob_mat=trans_prob_p0,\n",
    "                               expected_immediate_rew=R_ime_p0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(filename=\"Img/state_immdiate_p0.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Improve the policy by acting greedily with respect to $V^{\\pi_0}$ to get a new policy $\\pi_1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last part of the Policy iteration alghorithm is to improve the policy by acting greedily with respect to $V^{\\pi_0}$ to get a new policy $\\pi_1$.\n",
    "\n",
    "The greedy equation is heaviliy based on Bellman Equation, and in fact it is about finding the action with highest **State-Value** function for each state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\arg\\max_{a} \\left\\{ \\overline{R}(s,a) + \\gamma \\sum_{s' \\in \\mathcal{N}}\\mathbb{P}(S_{t+1}=s'|S_t=s, a_t=a)V(S_{t+1}=s') \\right\\}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the code for the greedy operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_operation(MDP_full, state_val_policy, old_policy, gamma=0.9):\n",
    "        # Perform the greedy operation to improve the policy\n",
    "        new_policy = {}\n",
    "        for state in old_policy.keys():\n",
    "            max_q_value, best_action  = float('-inf'), None\n",
    "            state_val_dict = state_val_policy.to_dict(orient=\"index\")\n",
    "            for action in MDP_full[state].keys():\n",
    "                q_value = 0\n",
    "                for (next_state, immediate_reward), probability in MDP_full[state][action].items():\n",
    "                    q_value = q_value +  probability * (immediate_reward + gamma *\n",
    "                        (state_val_dict[next_state][\"Value Function\"]))\n",
    "                if q_value > max_q_value:\n",
    "                    max_q_value, best_action = q_value, action\n",
    "            new_policy[state] = best_action\n",
    "        return new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_policy = greedy_operation(MDP_full=MDP_dict, \n",
    "                              state_val_policy=state_val_p0, \n",
    "                              old_policy=policy_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new policy after first itearation of the poliy iteration alghorithm is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Policy Iteration alghorthm, we keep iteration on the above three steps, until the policy converges. In other words, we keep iterating on the above three steps, until the policy stops to change. Here is the code for the Policy Iteration Alghorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration():\n",
    "        # Perform policy iteration to find the optimal policy\n",
    "        policy = policy_0_gen(user_capacity_val)\n",
    "        while True:\n",
    "            MRP_policy_p0 = MRP_using_fixedPolicy(MDP_dict, policy)\n",
    "            expected_immediate_rew = calculate_expected_immediate_rewards(MRP_policy_p0)\n",
    "            trans_prob_mat_val = create_transition_probability_matrix(MRP_policy_p0)\n",
    "            value_function = calculate_state_value_function(trans_prob_mat=trans_prob_mat_val,\n",
    "                                                            expected_immediate_rew=expected_immediate_rew,\n",
    "                                                            gamma=0.9)\n",
    "            new_policy = greedy_operation(MDP_full=MDP_dict, \n",
    "                                          state_val_policy=value_function, \n",
    "                                          old_policy=policy)\n",
    "            if new_policy == policy:\n",
    "                break\n",
    "            policy = new_policy\n",
    "        \n",
    "        opt_policy = new_policy\n",
    "        opt_value_func = value_function\n",
    "        \n",
    "        return opt_policy, opt_value_func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_policy, opt_val = policy_iteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the Optimal Order?\n",
    "\n",
    "Now, we can look on outcome of policy iteration and see what is the optimal order for each state. Here is the code for it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For state (0, 0), the optimal order quantity is: 1\n",
      "For state (0, 1), the optimal order quantity is: 1\n",
      "For state (0, 2), the optimal order quantity is: 0\n",
      "For state (1, 0), the optimal order quantity is: 1\n",
      "For state (1, 1), the optimal order quantity is: 0\n",
      "For state (2, 0), the optimal order quantity is: 0\n"
     ]
    }
   ],
   "source": [
    "for state, order_quantity in opt_policy.items():\n",
    "    print(f\"For state {state}, the optimal order quantity is: {order_quantity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting Everything Together: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import poisson\n",
    "import pandas as pd\n",
    "\n",
    "class MarkovDecisionProcess:\n",
    "    def __init__(self, user_capacity, poisson_lambda, holding_cost, stockout_cost, gamma):\n",
    "        # Initialize the MDP with given parameters\n",
    "        self.user_capacity = user_capacity\n",
    "        self.poisson_lambda = poisson_lambda\n",
    "        self.holding_cost, self.stockout_cost = holding_cost, stockout_cost\n",
    "        self.gamma = gamma\n",
    "        self.full_MDP = self.create_full_MDP()  # Create the full MDP\n",
    "\n",
    "    def create_full_MDP(self):\n",
    "        # Create the full MDP dictionary\n",
    "        MDP_dict = {}\n",
    "        for alpha in range(self.user_capacity + 1):\n",
    "            for beta in range(self.user_capacity + 1 - alpha):\n",
    "                state, init_inv = (alpha, beta), alpha + beta \n",
    "                action = {}\n",
    "                for order in range(self.user_capacity - init_inv + 1):\n",
    "                    dict1 = {}\n",
    "                    for i in range(init_inv + 1):\n",
    "                        if i <= (init_inv - 1):\n",
    "                            transition_prob = poisson.pmf(i, self.poisson_lambda)\n",
    "                            dict1[((init_inv - i, order), -alpha * self.holding_cost)] = transition_prob\n",
    "                        else:\n",
    "                            transition_prob = 1 - poisson.cdf(init_inv - 1, self.poisson_lambda)\n",
    "                            transition_prob2 = 1 - poisson.cdf(init_inv, self.poisson_lambda)\n",
    "                            reward = -alpha * self.holding_cost - self.stockout_cost * (\n",
    "                                (self.poisson_lambda * transition_prob) - init_inv * transition_prob2)\n",
    "                            dict1[((0, order), reward)] = transition_prob\n",
    "                    action[order] = dict1\n",
    "                MDP_dict[state] = action\n",
    "        return MDP_dict\n",
    "\n",
    "    def policy_0_gen(self):\n",
    "        # Generate an initial policy\n",
    "        return {(alpha, beta): self.user_capacity - (alpha + beta) \n",
    "                for alpha in range(self.user_capacity + 1) \n",
    "                for beta in range(self.user_capacity + 1 - alpha)}\n",
    "\n",
    "    def MRP_using_fixedPolicy(self, policy):\n",
    "        # Create the MRP using a fixed policy\n",
    "        return {state: self.full_MDP[state][action] \n",
    "                for state, action in policy.items()}\n",
    "    \n",
    "    def calculate_state_value_function(self, MRP_policy):\n",
    "        # Calculate the expected immediate rewards from the MRP policy\n",
    "        E_immediate_R = {}\n",
    "        for from_state, value in MRP_policy.items():\n",
    "            expected_reward = sum(reward[1] * prob for (reward, prob) in value.items())\n",
    "            E_immediate_R[from_state] = expected_reward\n",
    "\n",
    "        # Create the transition probability matrix\n",
    "        states = list(MRP_policy.keys())\n",
    "        trans_prob = np.zeros((len(states), len(states)))\n",
    "        df_trans_prob = pd.DataFrame(trans_prob, columns=states, index=states)\n",
    "        for i, from_state in enumerate(states):\n",
    "            for j, to_state in enumerate(states):\n",
    "                for (new_state, reward) in MRP_policy.get(from_state, {}):\n",
    "                    if new_state == to_state:\n",
    "                        probability = MRP_policy[from_state].get((new_state, reward), 0.0)\n",
    "                        df_trans_prob.iloc[i, j] = probability\n",
    "\n",
    "        # Calculate the state value function\n",
    "        R_exp = np.array(list(E_immediate_R.values()))\n",
    "        val_func_vec = np.linalg.solve(np.eye(len(R_exp)) - self.gamma * df_trans_prob, R_exp)\n",
    "        MarkRevData = pd.DataFrame({'Expected Immediate Reward': R_exp, 'Value Function': val_func_vec}, index=states)\n",
    "        return MarkRevData\n",
    "\n",
    "    def greedy_operation(self, MDP_full, state_val_policy, old_policy):\n",
    "        # Perform the greedy operation to improve the policy\n",
    "        new_policy = {}\n",
    "        for state in old_policy.keys():\n",
    "            max_q_value, best_action  = float('-inf'), None\n",
    "            state_val_dict = state_val_policy.to_dict(orient=\"index\")\n",
    "            for action in MDP_full[state].keys():\n",
    "                q_value = 0\n",
    "                for (next_state, immediate_reward), probability in MDP_full[state][action].items():\n",
    "                    q_value = q_value +  probability * (immediate_reward + self.gamma *\n",
    "                        (state_val_dict[next_state][\"Value Function\"]))\n",
    "                if q_value > max_q_value:\n",
    "                    max_q_value, best_action = q_value, action\n",
    "            new_policy[state] = best_action\n",
    "        return new_policy\n",
    "\n",
    "    def policy_iteration(self):\n",
    "        # Perform policy iteration to find the optimal policy\n",
    "        policy = self.policy_0_gen()\n",
    "        while True:\n",
    "            MRP_policy_p0 = self.MRP_using_fixedPolicy(policy)\n",
    "            value_function = self.calculate_state_value_function(MRP_policy_p0)\n",
    "            new_policy = self.greedy_operation(self.full_MDP, value_function, policy)\n",
    "            if new_policy == policy:\n",
    "                break\n",
    "            policy = new_policy\n",
    "        opt_policy, opt_value_func = new_policy, value_function\n",
    "        return opt_policy, opt_value_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m1\u001b[0m, \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m1\u001b[0m, \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m0\u001b[0m, \u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m1\u001b[0m, \u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m0\u001b[0m, \u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m0\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example usage:\n",
    "user_capacity = 2\n",
    "poisson_lambda = 1.0\n",
    "holding_cost = 1\n",
    "stockout_cost = 10\n",
    "gamma = 0.9\n",
    "\n",
    "MDP_Example = MarkovDecisionProcess(user_capacity, poisson_lambda, holding_cost, stockout_cost, gamma)\n",
    "\n",
    "opt_policy, opt_val = MDP_Example.policy_iteration()\n",
    "\n",
    "opt_policy\n",
    "#opt_val"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_finance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
